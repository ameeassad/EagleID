{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_embed.proj.weight: requires_grad = True\n",
      "patch_embed.proj.bias: requires_grad = True\n",
      "patch_embed.norm.weight: requires_grad = True\n",
      "patch_embed.norm.bias: requires_grad = True\n",
      "layers.0.blocks.0.norm1.weight: requires_grad = True\n",
      "layers.0.blocks.0.norm1.bias: requires_grad = True\n",
      "layers.0.blocks.0.attn.relative_position_bias_table: requires_grad = True\n",
      "layers.0.blocks.0.attn.qkv.weight: requires_grad = True\n",
      "layers.0.blocks.0.attn.qkv.bias: requires_grad = True\n",
      "layers.0.blocks.0.attn.proj.weight: requires_grad = True\n",
      "layers.0.blocks.0.attn.proj.bias: requires_grad = True\n",
      "layers.0.blocks.0.norm2.weight: requires_grad = True\n",
      "layers.0.blocks.0.norm2.bias: requires_grad = True\n",
      "layers.0.blocks.0.mlp.fc1.weight: requires_grad = True\n",
      "layers.0.blocks.0.mlp.fc1.bias: requires_grad = True\n",
      "layers.0.blocks.0.mlp.fc2.weight: requires_grad = True\n",
      "layers.0.blocks.0.mlp.fc2.bias: requires_grad = True\n",
      "layers.0.blocks.1.norm1.weight: requires_grad = True\n",
      "layers.0.blocks.1.norm1.bias: requires_grad = True\n",
      "layers.0.blocks.1.attn.relative_position_bias_table: requires_grad = True\n",
      "layers.0.blocks.1.attn.qkv.weight: requires_grad = True\n",
      "layers.0.blocks.1.attn.qkv.bias: requires_grad = True\n",
      "layers.0.blocks.1.attn.proj.weight: requires_grad = True\n",
      "layers.0.blocks.1.attn.proj.bias: requires_grad = True\n",
      "layers.0.blocks.1.norm2.weight: requires_grad = True\n",
      "layers.0.blocks.1.norm2.bias: requires_grad = True\n",
      "layers.0.blocks.1.mlp.fc1.weight: requires_grad = True\n",
      "layers.0.blocks.1.mlp.fc1.bias: requires_grad = True\n",
      "layers.0.blocks.1.mlp.fc2.weight: requires_grad = True\n",
      "layers.0.blocks.1.mlp.fc2.bias: requires_grad = True\n",
      "layers.1.downsample.norm.weight: requires_grad = True\n",
      "layers.1.downsample.norm.bias: requires_grad = True\n",
      "layers.1.downsample.reduction.weight: requires_grad = True\n",
      "layers.1.blocks.0.norm1.weight: requires_grad = True\n",
      "layers.1.blocks.0.norm1.bias: requires_grad = True\n",
      "layers.1.blocks.0.attn.relative_position_bias_table: requires_grad = True\n",
      "layers.1.blocks.0.attn.qkv.weight: requires_grad = True\n",
      "layers.1.blocks.0.attn.qkv.bias: requires_grad = True\n",
      "layers.1.blocks.0.attn.proj.weight: requires_grad = True\n",
      "layers.1.blocks.0.attn.proj.bias: requires_grad = True\n",
      "layers.1.blocks.0.norm2.weight: requires_grad = True\n",
      "layers.1.blocks.0.norm2.bias: requires_grad = True\n",
      "layers.1.blocks.0.mlp.fc1.weight: requires_grad = True\n",
      "layers.1.blocks.0.mlp.fc1.bias: requires_grad = True\n",
      "layers.1.blocks.0.mlp.fc2.weight: requires_grad = True\n",
      "layers.1.blocks.0.mlp.fc2.bias: requires_grad = True\n",
      "layers.1.blocks.1.norm1.weight: requires_grad = True\n",
      "layers.1.blocks.1.norm1.bias: requires_grad = True\n",
      "layers.1.blocks.1.attn.relative_position_bias_table: requires_grad = True\n",
      "layers.1.blocks.1.attn.qkv.weight: requires_grad = True\n",
      "layers.1.blocks.1.attn.qkv.bias: requires_grad = True\n",
      "layers.1.blocks.1.attn.proj.weight: requires_grad = True\n",
      "layers.1.blocks.1.attn.proj.bias: requires_grad = True\n",
      "layers.1.blocks.1.norm2.weight: requires_grad = True\n",
      "layers.1.blocks.1.norm2.bias: requires_grad = True\n",
      "layers.1.blocks.1.mlp.fc1.weight: requires_grad = True\n",
      "layers.1.blocks.1.mlp.fc1.bias: requires_grad = True\n",
      "layers.1.blocks.1.mlp.fc2.weight: requires_grad = True\n",
      "layers.1.blocks.1.mlp.fc2.bias: requires_grad = True\n",
      "layers.2.downsample.norm.weight: requires_grad = True\n",
      "layers.2.downsample.norm.bias: requires_grad = True\n",
      "layers.2.downsample.reduction.weight: requires_grad = True\n",
      "layers.2.blocks.0.norm1.weight: requires_grad = True\n",
      "layers.2.blocks.0.norm1.bias: requires_grad = True\n",
      "layers.2.blocks.0.attn.relative_position_bias_table: requires_grad = True\n",
      "layers.2.blocks.0.attn.qkv.weight: requires_grad = True\n",
      "layers.2.blocks.0.attn.qkv.bias: requires_grad = True\n",
      "layers.2.blocks.0.attn.proj.weight: requires_grad = True\n",
      "layers.2.blocks.0.attn.proj.bias: requires_grad = True\n",
      "layers.2.blocks.0.norm2.weight: requires_grad = True\n",
      "layers.2.blocks.0.norm2.bias: requires_grad = True\n",
      "layers.2.blocks.0.mlp.fc1.weight: requires_grad = True\n",
      "layers.2.blocks.0.mlp.fc1.bias: requires_grad = True\n",
      "layers.2.blocks.0.mlp.fc2.weight: requires_grad = True\n",
      "layers.2.blocks.0.mlp.fc2.bias: requires_grad = True\n",
      "layers.2.blocks.1.norm1.weight: requires_grad = True\n",
      "layers.2.blocks.1.norm1.bias: requires_grad = True\n",
      "layers.2.blocks.1.attn.relative_position_bias_table: requires_grad = True\n",
      "layers.2.blocks.1.attn.qkv.weight: requires_grad = True\n",
      "layers.2.blocks.1.attn.qkv.bias: requires_grad = True\n",
      "layers.2.blocks.1.attn.proj.weight: requires_grad = True\n",
      "layers.2.blocks.1.attn.proj.bias: requires_grad = True\n",
      "layers.2.blocks.1.norm2.weight: requires_grad = True\n",
      "layers.2.blocks.1.norm2.bias: requires_grad = True\n",
      "layers.2.blocks.1.mlp.fc1.weight: requires_grad = True\n",
      "layers.2.blocks.1.mlp.fc1.bias: requires_grad = True\n",
      "layers.2.blocks.1.mlp.fc2.weight: requires_grad = True\n",
      "layers.2.blocks.1.mlp.fc2.bias: requires_grad = True\n",
      "layers.2.blocks.2.norm1.weight: requires_grad = True\n",
      "layers.2.blocks.2.norm1.bias: requires_grad = True\n",
      "layers.2.blocks.2.attn.relative_position_bias_table: requires_grad = True\n",
      "layers.2.blocks.2.attn.qkv.weight: requires_grad = True\n",
      "layers.2.blocks.2.attn.qkv.bias: requires_grad = True\n",
      "layers.2.blocks.2.attn.proj.weight: requires_grad = True\n",
      "layers.2.blocks.2.attn.proj.bias: requires_grad = True\n",
      "layers.2.blocks.2.norm2.weight: requires_grad = True\n",
      "layers.2.blocks.2.norm2.bias: requires_grad = True\n",
      "layers.2.blocks.2.mlp.fc1.weight: requires_grad = True\n",
      "layers.2.blocks.2.mlp.fc1.bias: requires_grad = True\n",
      "layers.2.blocks.2.mlp.fc2.weight: requires_grad = True\n",
      "layers.2.blocks.2.mlp.fc2.bias: requires_grad = True\n",
      "layers.2.blocks.3.norm1.weight: requires_grad = True\n",
      "layers.2.blocks.3.norm1.bias: requires_grad = True\n",
      "layers.2.blocks.3.attn.relative_position_bias_table: requires_grad = True\n",
      "layers.2.blocks.3.attn.qkv.weight: requires_grad = True\n",
      "layers.2.blocks.3.attn.qkv.bias: requires_grad = True\n",
      "layers.2.blocks.3.attn.proj.weight: requires_grad = True\n",
      "layers.2.blocks.3.attn.proj.bias: requires_grad = True\n",
      "layers.2.blocks.3.norm2.weight: requires_grad = True\n",
      "layers.2.blocks.3.norm2.bias: requires_grad = True\n",
      "layers.2.blocks.3.mlp.fc1.weight: requires_grad = True\n",
      "layers.2.blocks.3.mlp.fc1.bias: requires_grad = True\n",
      "layers.2.blocks.3.mlp.fc2.weight: requires_grad = True\n",
      "layers.2.blocks.3.mlp.fc2.bias: requires_grad = True\n",
      "layers.2.blocks.4.norm1.weight: requires_grad = True\n",
      "layers.2.blocks.4.norm1.bias: requires_grad = True\n",
      "layers.2.blocks.4.attn.relative_position_bias_table: requires_grad = True\n",
      "layers.2.blocks.4.attn.qkv.weight: requires_grad = True\n",
      "layers.2.blocks.4.attn.qkv.bias: requires_grad = True\n",
      "layers.2.blocks.4.attn.proj.weight: requires_grad = True\n",
      "layers.2.blocks.4.attn.proj.bias: requires_grad = True\n",
      "layers.2.blocks.4.norm2.weight: requires_grad = True\n",
      "layers.2.blocks.4.norm2.bias: requires_grad = True\n",
      "layers.2.blocks.4.mlp.fc1.weight: requires_grad = True\n",
      "layers.2.blocks.4.mlp.fc1.bias: requires_grad = True\n",
      "layers.2.blocks.4.mlp.fc2.weight: requires_grad = True\n",
      "layers.2.blocks.4.mlp.fc2.bias: requires_grad = True\n",
      "layers.2.blocks.5.norm1.weight: requires_grad = True\n",
      "layers.2.blocks.5.norm1.bias: requires_grad = True\n",
      "layers.2.blocks.5.attn.relative_position_bias_table: requires_grad = True\n",
      "layers.2.blocks.5.attn.qkv.weight: requires_grad = True\n",
      "layers.2.blocks.5.attn.qkv.bias: requires_grad = True\n",
      "layers.2.blocks.5.attn.proj.weight: requires_grad = True\n",
      "layers.2.blocks.5.attn.proj.bias: requires_grad = True\n",
      "layers.2.blocks.5.norm2.weight: requires_grad = True\n",
      "layers.2.blocks.5.norm2.bias: requires_grad = True\n",
      "layers.2.blocks.5.mlp.fc1.weight: requires_grad = True\n",
      "layers.2.blocks.5.mlp.fc1.bias: requires_grad = True\n",
      "layers.2.blocks.5.mlp.fc2.weight: requires_grad = True\n",
      "layers.2.blocks.5.mlp.fc2.bias: requires_grad = True\n",
      "layers.3.downsample.norm.weight: requires_grad = True\n",
      "layers.3.downsample.norm.bias: requires_grad = True\n",
      "layers.3.downsample.reduction.weight: requires_grad = True\n",
      "layers.3.blocks.0.norm1.weight: requires_grad = True\n",
      "layers.3.blocks.0.norm1.bias: requires_grad = True\n",
      "layers.3.blocks.0.attn.relative_position_bias_table: requires_grad = True\n",
      "layers.3.blocks.0.attn.qkv.weight: requires_grad = True\n",
      "layers.3.blocks.0.attn.qkv.bias: requires_grad = True\n",
      "layers.3.blocks.0.attn.proj.weight: requires_grad = True\n",
      "layers.3.blocks.0.attn.proj.bias: requires_grad = True\n",
      "layers.3.blocks.0.norm2.weight: requires_grad = True\n",
      "layers.3.blocks.0.norm2.bias: requires_grad = True\n",
      "layers.3.blocks.0.mlp.fc1.weight: requires_grad = True\n",
      "layers.3.blocks.0.mlp.fc1.bias: requires_grad = True\n",
      "layers.3.blocks.0.mlp.fc2.weight: requires_grad = True\n",
      "layers.3.blocks.0.mlp.fc2.bias: requires_grad = True\n",
      "layers.3.blocks.1.norm1.weight: requires_grad = True\n",
      "layers.3.blocks.1.norm1.bias: requires_grad = True\n",
      "layers.3.blocks.1.attn.relative_position_bias_table: requires_grad = True\n",
      "layers.3.blocks.1.attn.qkv.weight: requires_grad = True\n",
      "layers.3.blocks.1.attn.qkv.bias: requires_grad = True\n",
      "layers.3.blocks.1.attn.proj.weight: requires_grad = True\n",
      "layers.3.blocks.1.attn.proj.bias: requires_grad = True\n",
      "layers.3.blocks.1.norm2.weight: requires_grad = True\n",
      "layers.3.blocks.1.norm2.bias: requires_grad = True\n",
      "layers.3.blocks.1.mlp.fc1.weight: requires_grad = True\n",
      "layers.3.blocks.1.mlp.fc1.bias: requires_grad = True\n",
      "layers.3.blocks.1.mlp.fc2.weight: requires_grad = True\n",
      "layers.3.blocks.1.mlp.fc2.bias: requires_grad = True\n",
      "norm.weight: requires_grad = True\n",
      "norm.bias: requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n",
    "for name, param in backbone.named_parameters():\n",
    "    print(f\"{name}: requires_grad = {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import argparse\n",
    "import shutil\n",
    "import os\n",
    "import yaml\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_lightning import Trainer\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import wandb\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wildlife_tools.features import DeepFeatures\n",
    "\n",
    "backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 768\n"
     ]
    }
   ],
   "source": [
    "backbone = backbone.eval()\n",
    "\n",
    "# Generate a dummy batch of images with a shape of (batch_size, channels, height, width)\n",
    "# Typical input size for Swin Transformer models might be (1, 3, 224, 224)\n",
    "dummy_batch = torch.randn(1, 3, 224, 224)  # Change 224 to your model’s input size if different\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = backbone(dummy_batch)\n",
    "\n",
    "# Output shape will be (1, dim_embedding)\n",
    "print(\"Embedding size:\", output.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amee/miniconda3/envs/pytorch_env/lib/python3.12/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from models.triplet_loss_model import TripletModel\n",
    "\n",
    "from utils.re_ranking import re_ranking\n",
    "from data.data_utils import calculate_num_channels\n",
    "from utils.metrics import compute_distance_matrix\n",
    "from utils.metrics import evaluate_map, compute_average_precision\n",
    "\n",
    "# model = TripletModel(backbone_model_name=backbone)\n",
    "\n",
    "# Embedder (to project features into the desired embedding space)\n",
    "# embedder = nn.Linear(backbone.feature_info[-1][\"num_chs\"], 768)\n",
    "\n",
    "re_ranking = True\n",
    "distance_matrix = 'euclidean'\n",
    "query_embeddings = []\n",
    "query_labels = []\n",
    "gallery_embeddings = []\n",
    "gallery_labels = []\n",
    "\n",
    "def validation_step(batch, batch_idx, dataloader_idx=0):\n",
    "    x, target = batch\n",
    "    embeddings = backbone(x)\n",
    "    if dataloader_idx == 0:\n",
    "        # Query data\n",
    "        query_embeddings.append(embeddings)\n",
    "        query_labels.append(target)\n",
    "    else:\n",
    "        # Gallery data\n",
    "        gallery_embeddings.append(embeddings)\n",
    "        gallery_labels.append(target)\n",
    "\n",
    "def on_validation_epoch_end():\n",
    "    # Concatenate all embeddings and labels\n",
    "    query_embeddings = torch.cat(query_embeddings)\n",
    "    query_labels = torch.cat(query_labels)\n",
    "    gallery_embeddings = torch.cat(gallery_embeddings)\n",
    "    gallery_labels = torch.cat(gallery_labels)\n",
    "\n",
    "    # Compute distance matrix\n",
    "    if re_ranking:\n",
    "        distmat = re_ranking(query_embeddings, gallery_embeddings, k1=20, k2=6, lambda_value=0.3)\n",
    "    else:\n",
    "        distmat = compute_distance_matrix(distance_matrix, query_embeddings, gallery_embeddings, wildlife=True)\n",
    "\n",
    "    # Compute mAP\n",
    "    # mAP = torchreid.metrics.evaluate_rank(distmat, query_labels.cpu().numpy(), gallery_labels.cpu().numpy(), use_cython=False)[0]['mAP']\n",
    "    mAP1 = evaluate_map(distmat, query_labels, gallery_labels, top_k=1)\n",
    "    mAP5 = evaluate_map(distmat, query_labels, gallery_labels, top_k=5)\n",
    "    print(mAP5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils.triplet_loss_utils import TripletLoss\n",
    "from utils.optimizer import get_optimizer, get_lr_scheduler_config\n",
    "from utils.weights_initializer import weights_init_kaiming, weights_init_classifier\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_metric_learning import losses, miners\n",
    "from torch import nn\n",
    "\n",
    "from wildlife_tools.similarity.cosine import CosineSimilarity\n",
    "from utils.metrics import evaluate_map, compute_average_precision\n",
    "\n",
    "from utils.re_ranking import re_ranking\n",
    "from data.data_utils import calculate_num_channels\n",
    "from utils.metrics import compute_distance_matrix\n",
    "from utils.triplet_loss_utils import KnnClassifier\n",
    "\n",
    "\n",
    "class SimpleModel(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 backbone_model_name=\"resnet50\", \n",
    "                 config=None, pretrained=True, \n",
    "                 embedding_size=768, margin=0.2, \n",
    "                 mining_type=\"semihard\", \n",
    "                 lr=0.001, \n",
    "                 preprocess_lvl=0, \n",
    "                 re_ranking=True, \n",
    "                 outdir=\"results\"):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.re_ranking = re_ranking\n",
    "        self.distance_matrix = 'cosine'\n",
    "            \n",
    "        # Backbone (ResNet without the final FC layer)\n",
    "        self.backbone = timm.create_model(model_name=backbone_model_name, pretrained=pretrained, num_classes=0)\n",
    "\n",
    "        self.embedder = nn.Linear(self.backbone.feature_info[-1][\"num_chs\"], embedding_size)\n",
    "\n",
    "    # Can experiment with different embedders or need to adjust the embedding layer frequently.\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x) # Extract features using the backbone\n",
    "        return features\n",
    "        embeddings = self.embedder(features) # Project features into the embedding space\n",
    "        embeddings = nn.functional.normalize(embeddings, p=2, dim=1)  # L2 normalization\n",
    "        return embeddings\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        embeddings = self(images)\n",
    "        mined_triplets = self.miner(embeddings, labels)\n",
    "        loss = self.loss_fn(embeddings, labels, mined_triplets)\n",
    "        self.log(\"train/loss\", loss,  on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.query_embeddings = []\n",
    "        self.query_labels = []\n",
    "        self.gallery_embeddings = []\n",
    "        self.gallery_labels = []\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, target = batch\n",
    "        embeddings = self(x)\n",
    "        if dataloader_idx == 0:\n",
    "            # Query data\n",
    "            self.query_embeddings.append(embeddings)\n",
    "            self.query_labels.append(target)\n",
    "        else:\n",
    "            # Gallery data\n",
    "            self.gallery_embeddings.append(embeddings)\n",
    "            self.gallery_labels.append(target)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Concatenate all embeddings and labels\n",
    "        query_embeddings = torch.cat(self.query_embeddings)\n",
    "        query_labels = torch.cat(self.query_labels)\n",
    "        gallery_embeddings = torch.cat(self.gallery_embeddings)\n",
    "        gallery_labels = torch.cat(self.gallery_labels)\n",
    "\n",
    "        print(f\"size of gallery embeddings: {gallery_embeddings.size()}\")\n",
    "        print(f\"size of gallery labels: {gallery_labels.size()}\")\n",
    "        print(f\"size of query embeddings: {query_embeddings.size()}\")\n",
    "        print(f\"size of query labels: {query_labels.size()}\")\n",
    "\n",
    "        # Compute distance matrix\n",
    "        if self.re_ranking:\n",
    "            distmat = re_ranking(query_embeddings, gallery_embeddings, k1=20, k2=6, lambda_value=0.3)\n",
    "        else:\n",
    "            distmat = compute_distance_matrix(self.distance_matrix, query_embeddings, gallery_embeddings, wildlife=True)\n",
    "\n",
    "        # Compute mAP\n",
    "        # mAP = torchreid.metrics.evaluate_rank(distmat, query_labels.cpu().numpy(), gallery_labels.cpu().numpy(), use_cython=False)[0]['mAP']\n",
    "        mAP1 = evaluate_map(distmat, query_labels, gallery_labels, top_k=1)\n",
    "        mAP5 = evaluate_map(distmat, query_labels, gallery_labels, top_k=5)\n",
    "        self.log('val/mAP1', mAP1)\n",
    "        self.log('val/mAP5', mAP5)\n",
    "\n",
    "        similarity_function = CosineSimilarity()\n",
    "        similarity = similarity_function(query_embeddings, gallery_embeddings)[\"cosine\"]\n",
    "        print(\"Similarity matrix: \\n\", similarity.shape)\n",
    "        print(similarity)\n",
    "\n",
    "        # Convert gallery_labels to numpy if necessary\n",
    "        gallery_labels = gallery_labels.cpu().numpy() if isinstance(gallery_labels, torch.Tensor) else gallery_labels\n",
    "        print(\"Gallery labels: \\n\", gallery_labels)\n",
    "        print(\"Query labels: \\n\", query_labels)\n",
    "\n",
    "        query_map = [i for i in range(len(query_labels))]\n",
    "\n",
    "        # Nearest neighbor classifier using KNN with k=1\n",
    "        classifier = KnnClassifier(k=1)\n",
    "        preds = classifier(similarity)\n",
    "        print(f\"preds: {preds}\")\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = (preds == query_labels.cpu().numpy()).mean()\n",
    "        print(f\"accuracy: {accuracy}\")\n",
    "        self.log('val/accuracy', accuracy)\n",
    "\n",
    "        # # Calculate Recall@K (choose K=5)\n",
    "        # K = 5\n",
    "        # top_k_preds = np.argsort(-similarity, axis=1)[:, :K]  # Get top K indices for each query\n",
    "        # recall_at_k = 0\n",
    "        # for i, query_label in enumerate(query_labels):\n",
    "        #     top_k_labels = gallery_labels[top_k_preds[i]]  # Get top K labels for the query\n",
    "        #     if query_label in top_k_labels:\n",
    "        #         recall_at_k += 1\n",
    "\n",
    "        # # Log Recall@K\n",
    "        # recall_at_k /= len(query_labels)\n",
    "        # self.log(f'val/Recall@{K}', recall_at_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel(backbone_model_name='hf-hub:BVRA/MegaDescriptor-T-224',\n",
    "                    re_ranking=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: time-unaware closed-set\n",
      "Samples: train/test/unassigned/total = 6108/1585/0/7693\n",
      "Classes: train/test/unassigned/total = 543/512/0/543\n",
      "Samples: train only/test only        = 31/0\n",
      "Classes: train only/test only/joint  = 31/0/512\n",
      "\n",
      "Fraction of train set     = 79.40%\n",
      "Fraction of test set only = 0.00%\n",
      "Train set size: 6108\n",
      "Test set size: 1585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amee/miniconda3/envs/pytorch_env/lib/python3.12/site-packages/ultralytics/nn/tasks.py:732: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(file, map_location=\"cpu\")\n",
      "/Users/amee/miniconda3/envs/pytorch_env/lib/python3.12/site-packages/ultralytics/nn/tasks.py:732: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No segmentation data found: Empty list or list containing empty list.\n",
      "Removed 1 rows with invalid segmentation data.\n",
      "Removed 0 rows with invalid segmentation data.\n",
      "Training Set\n",
      "Length: 2090\n",
      "Number of individuals: 315\n",
      "Mean images/individual: 6.634920634920635\n",
      "Min images/individual: 2\n",
      "Max images/individual: 50\n",
      "Test Set\n",
      "Length: 584\n",
      "Number of individuals: 295\n",
      "Mean images per individual: 1.9796610169491526\n",
      "Min images per individual: 1\n",
      "Max images per individual: 18\n",
      "length of query dataset: 125\n",
      "length of gallery dataset: 459\n"
     ]
    }
   ],
   "source": [
    "from wildlife_datasets import analysis, datasets, loader\n",
    "from data.raptors_wildlife import Raptors, WildlifeReidDataModule\n",
    "\n",
    "root = '/Users/amee/Documents/code/master-thesis/datasets/EDA-whaleshark/'\n",
    "\n",
    "dataset = datasets.WhaleSharkID(root)\n",
    "data = WildlifeReidDataModule(data_dir=root, \n",
    "                              metadata=dataset.df, \n",
    "                              cache_path='/Users/amee/Documents/code/master-thesis/EagleID/dataset/dataframe/cache_whaleshark.csv', \n",
    "                              size = 224, \n",
    "                              preprocess_lvl=1, \n",
    "                              only_cache=True, \n",
    "                              batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/amee/miniconda3/envs/pytorch_env/lib/python3.12/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/amee/miniconda3/envs/pytorch_env/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc7eeb7110e4dcd95725fc498693907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of gallery embeddings: torch.Size([459, 768])\n",
      "size of gallery labels: torch.Size([459])\n",
      "size of query embeddings: torch.Size([125, 768])\n",
      "size of query labels: torch.Size([125])\n",
      "Distance matrix type should be np for rerankin: <class 'numpy.ndarray'>\n",
      "Similarity matrix: \n",
      " (125, 459)\n",
      "[[    0.48836     0.41789     0.71838 ...     0.37283     0.66797     0.28333]\n",
      " [    0.18126     0.26546     0.77685 ...     0.12444     0.73263     0.11917]\n",
      " [    0.55198     0.54416     0.53006 ...     0.43508     0.45751     0.22246]\n",
      " ...\n",
      " [    0.14237     0.16995     0.72094 ...     0.18472     0.56607      0.1169]\n",
      " [    0.74552     0.48677     0.28043 ...     0.49063     0.33863     0.27755]\n",
      " [    0.61553     0.39169     0.47036 ...     0.72542      0.3144     0.19776]]\n",
      "Gallery labels: \n",
      " [  0   1   2   2   3   4   5   6   7   8   8   9  10  11  11  11  12  12  13  14  15  16  17  18  19  20  21  21  22  23  24  24  24  24  25  26  26  26  26  26  26  26  27  28  29  30  31  32  32  33  34  35  36  37  38  39  39  39  39  39  39  39  39  39  40  41  42  43  43  43  43  44  45  45  45  45  46  47  47\n",
      "  48  48  48  48  48  48  49  50  51  52  53  53  54  55  56  57  58  59  60  61  62  63  63  63  63  63  63  63  63  63  63  63  63  63  63  63  63  63  64  65  66  66  67  68  69  70  71  72  72  73  74  74  74  74  74  74  74  74  74  75  76  77  77  78  79  79  79  79  79  79  79  79  79  79  79  79  80  81  82\n",
      "  83  84  84  85  86  87  88  89  90  91  91  92  93  94  94  94  94  95  96  97  97  98  98  99 100 101 102 103 103 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 117 118 119 120 120 121 122 122 122 122 122 122 123 124 124 124 124 124 124 124 124 125 126 127 128 128 128 129 130 131 131 132 133 134 134\n",
      " 135 136 136 137 138 139 140 141 141 141 142 142 142 142 143 144 145 146 147 147 147 148 149 150 151 152 152 152 153 154 155 155 156 157 158 159 160 161 162 163 164 164 165 165 166 167 168 169 170 171 171 171 172 173 174 174 175 176 177 178 179 180 181 182 182 182 182 182 182 182 182 183 184 184 184 185 186 186 186\n",
      " 187 187 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 210 211 212 213 214 215 216 217 217 218 219 220 221 222 223 224 225 226 227 228 228 229 230 231 232 233 234 235 236 237 237 238 239 240 241 242 243 244 245 246 247 247 247 247 248 249 250 251 251 252 253 254 255\n",
      " 256 257 258 259 260 260 261 261 262 262 262 262 263 263 263 264 265 266 266 266 267 268 269 269 270 271 272 273 274 275 276 277 277 278 279 279 280 281 281 281 282 282 282 282 282 283 283 283 283 284 285 286 287 288 289 290 290 290 291 292 292 292 293 294]\n",
      "Query labels: \n",
      " tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,\n",
      "         62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124])\n",
      "preds: [156 289 395 281 351 330  16   7  20  21  19 173 277 313 329 238 376  47 352 369  99  58  75 434 417  77 346 258 313 290 312 457 278 119 232 168 342 278 156 454  61 308 444 171  79  16 179 120 302 313  35  52  70 352  83 332  84 397 284 250 340 206 380  70 285  93  61 220  21  41  48  33 263 408 408 285 285 187 430\n",
      " 176 238 220 110 204 300 205  99 268  96 330 299 420 112 361 371 235 116 384 302 161 371 281 381 193  11 284 220 234 210 261 415 182 238 300 336 361 371 397 224 352 364  89 232 387 254]\n",
      "accuracy: 0.008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amee/miniconda3/envs/pytorch_env/lib/python3.12/site-packages/wildlife_tools/similarity/cosine.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a, b = torch.tensor(a), torch.tensor(b)\n",
      "/Users/amee/Documents/code/master-thesis/EagleID/notebooks/../utils/triplet_loss_utils.py:80: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  results = pd.DataFrame(results).T.fillna(method=\"ffill\").T\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃<span style=\"font-weight: bold\">       DataLoader 1        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val/accuracy        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.00800000037997961    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.00800000037997961    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val/mAP1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val/mAP5          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "└───────────────────────────┴───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 1       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val/accuracy       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.00800000037997961   \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.00800000037997961   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val/mAP1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val/mAP5         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val/mAP1': 0.0, 'val/mAP5': 0.0, 'val/accuracy': 0.00800000037997961},\n",
       " {'val/mAP1': 0.0, 'val/mAP5': 0.0, 'val/accuracy': 0.00800000037997961}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(accelerator=\"cpu\")\n",
    "trainer.validate(model, dataloaders=data.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:22<00:00, 22.07s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 4/4 [00:46<00:00, 11.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query features shape: (125, 768), Database features shape: (459, 768)\n",
      "Similarity matrix: \n",
      " (125, 459)\n",
      "Database labels size: 295\n",
      "Sample indices from similarity: [156 289 395 281 351 330  16   7  20  21]\n",
      "size of gallery dataset labels: 459\n",
      "125\n",
      "459\n",
      "Prediction \t ['4507ee90-84b6-fecb-cd98-2fbddd8707fb' '9cc4a537-c0a2-3d06-3780-6b327d292303' 'de473d56-8c61-13cb-6684-933726b40a3d' '96583bb9-245a-e067-da52-deca5b665631' 'bb27ae2e-1c8a-9a2b-e28f-eb8d05360bda' 'b2bb892c-9e2d-0ceb-a7a6-d81c90b937cf' '0d0160ac-3076-83fc-76ca-7bf0189d86e1' '0749736f-ca90-ed15-a460-1488f8ad9522'\n",
      " '0f46e82c-b6a7-5819-9852-003f6861922c' '0fca832e-87ea-029e-4a39-0c1ae5a49416' '0e35135a-e5a5-ac85-1867-78d694cc3d86' '535ee47e-9a8d-156b-439e-00ceb0bc08ec' '95d65d3c-d851-6fdf-2d80-b91ef84459cf' 'a9429b5d-fe9f-08f9-3f79-c6497a00fcfc' 'b1a459de-6869-191f-3650-c345bd1552d4' '77ec3c66-4342-b777-89fd-79f15a5f0c6b'\n",
      " 'cf744cd1-16d2-45c6-bd09-7149f9143159' '1cf57545-7a91-ab69-4ecc-5ef84fadd959' 'bc57a667-372a-587e-7c48-a3036c1bc7e8' 'c807ea87-84b8-e26a-0d45-b257940557eb' '33ef3e22-b625-972f-d3be-724063d21230' '21027863-1d99-20d5-e7f9-dd3f7ad3b166' '2583f9b6-1b10-2c5f-2136-62badc34dcf2' 'efbedb66-d0e1-a75c-61ed-e37b664b3031'\n",
      " 'e7515a92-d708-9705-8da7-34dbc4ef990d' '262a2330-5e47-8311-180d-920d61e42841' 'b9b7da46-e7da-c88f-c649-9645aa3431d7' '86ba25a6-d181-7057-bb2d-20e2f1181ee0' 'a9429b5d-fe9f-08f9-3f79-c6497a00fcfc' '9e34f8f4-1421-7cf3-6e3d-3526e9d0a9d6' 'a8b217e1-ff7c-fb10-ed6e-97929889ce74' 'fe6fe528-89b6-5d7c-8cc3-80f44e3f37f1'\n",
      " '95d65d3c-d851-6fdf-2d80-b91ef84459cf' '35aa73bf-5bdc-5647-0126-d67cf3b3fa7a' '7368cb71-43d8-450e-aacd-0863bdd88c02' '4fe4aa19-11cb-fa79-5d7e-43b2014dc286' 'b80f199f-2ead-a809-6a2d-1cf9fccd37a3' '95d65d3c-d851-6fdf-2d80-b91ef84459cf' '4507ee90-84b6-fecb-cd98-2fbddd8707fb' 'fd39063f-b353-c2af-99a7-0c87114472c9'\n",
      " '21027863-1d99-20d5-e7f9-dd3f7ad3b166' 'a7948cc2-9030-b74c-1514-bf47519e98cd' 'f1fd4f4c-18d2-dc56-c168-0b31a637e93a' '535ee47e-9a8d-156b-439e-00ceb0bc08ec' '26560de1-6930-ddaf-5069-f7b85acd40fb' '0d0160ac-3076-83fc-76ca-7bf0189d86e1' '56b9a02f-2373-b64c-ed63-5ac7cbf788bf' '35aa73bf-5bdc-5647-0126-d67cf3b3fa7a'\n",
      " 'a785af89-b8c0-5e7b-acec-c4874ec5483f' 'a9429b5d-fe9f-08f9-3f79-c6497a00fcfc' '18b0f340-8147-3d7f-a8aa-22d590108a1b' '1f8d3e82-b841-5def-bc2c-e87c8ddf0dd7' '24e404dd-094c-e5bd-defa-e52125422443' 'bc57a667-372a-587e-7c48-a3036c1bc7e8' '26560de1-6930-ddaf-5069-f7b85acd40fb' 'b4c1eb49-19c0-e405-612f-e372e2820b09'\n",
      " '26560de1-6930-ddaf-5069-f7b85acd40fb' 'e051e412-0fa0-cb8e-17e9-e53e72533a61' '986f211d-25e5-55f1-6c3c-af3e5f21fa7c' '7c8a623d-c9cf-0844-f844-b2ed52063b87' 'b7f6cea4-6818-4343-da7f-cbc004e83c8f' '672dbdf0-ace0-4c81-6e45-25ab615fe9e9' 'd4f14f4b-6899-fca9-c5df-0b658a7d2bde' '24e404dd-094c-e5bd-defa-e52125422443'\n",
      " '996d941d-79fa-885a-e12b-00200c7e61f0' '2c28b671-38f8-554d-4291-1bf2b895e10a' '21027863-1d99-20d5-e7f9-dd3f7ad3b166' '69f95579-8a42-8d93-8d78-7b641952933c' '0fca832e-87ea-029e-4a39-0c1ae5a49416' '18b0f340-8147-3d7f-a8aa-22d590108a1b' '1cf57545-7a91-ab69-4ecc-5ef84fadd959' '176a0208-db4d-26fc-5578-f38449c27981'\n",
      " '8b066c82-4140-4c42-c35a-12da4cd1ae17' 'e376b53f-b639-dcfb-742b-a6caf8494c42' 'e376b53f-b639-dcfb-742b-a6caf8494c42' '996d941d-79fa-885a-e12b-00200c7e61f0' '996d941d-79fa-885a-e12b-00200c7e61f0' '59af5b4c-cd5c-9d48-0f2c-7c390ffec1c3' 'ee21808a-5a63-c64d-d8f9-744d685abb8c' '55d7f55c-6be0-79a5-8726-69b723afbbae'\n",
      " '77ec3c66-4342-b777-89fd-79f15a5f0c6b' '69f95579-8a42-8d93-8d78-7b641952933c' '341569f2-1f34-4884-1dd3-79137be4c77f' '66755038-af33-95c4-2ee0-6f280843e658' 'a785af89-b8c0-5e7b-acec-c4874ec5483f' '672dbdf0-ace0-4c81-6e45-25ab615fe9e9' '33ef3e22-b625-972f-d3be-724063d21230' '8c85bd9e-b53d-38f5-9ae5-e6bb42cf939f'\n",
      " '325b5803-e2e3-9b4c-4289-8d12cc70337d' 'b2bb892c-9e2d-0ceb-a7a6-d81c90b937cf' 'a55cc64a-52c9-482c-5ed7-d7d843009f66' 'e91aa226-3080-5a21-169d-e5161adda406' '341569f2-1f34-4884-1dd3-79137be4c77f' 'c0ff5120-0537-81d2-6616-d7852ac605bf' 'c971740e-f359-97ec-aed3-1773e5e6bbac' '75e0b508-e0b7-0883-885c-ef049b83defb'\n",
      " '341569f2-1f34-4884-1dd3-79137be4c77f' 'd638c399-ff9f-b83d-3fe3-e533b881aa1d' 'a785af89-b8c0-5e7b-acec-c4874ec5483f' '4b0262ab-61cc-fdbf-8a08-320c9052963c' 'c971740e-f359-97ec-aed3-1773e5e6bbac' '96583bb9-245a-e067-da52-deca5b665631' 'd5715a5e-79ac-6d84-b946-39bcb8085a6f' '5e8ac347-9fa5-f8ed-9f15-6ada5cb15723'\n",
      " '0bf70dba-2bd4-39dd-48a9-227ab6f48bc0' '986f211d-25e5-55f1-6c3c-af3e5f21fa7c' '69f95579-8a42-8d93-8d78-7b641952933c' '74a2505c-fed2-9f90-e7f7-77b87a692081' '687318c6-fe9c-3f7f-8983-3c61cb5b6811' '8a5af3a0-f4ec-e914-9857-a6aa7b0582c9' 'e533ee6d-d7cb-a869-b8b1-1ff938c99db6' '57bc1889-ae33-3d86-cc7f-035e9faf6ba5'\n",
      " '77ec3c66-4342-b777-89fd-79f15a5f0c6b' 'a785af89-b8c0-5e7b-acec-c4874ec5483f' 'b5c9a7e3-4f56-b420-4b22-4919195bcddd' 'c0ff5120-0537-81d2-6616-d7852ac605bf' 'c971740e-f359-97ec-aed3-1773e5e6bbac' 'e051e412-0fa0-cb8e-17e9-e53e72533a61' '6d0d6c2d-6c32-fe7f-d92d-cf2681258d54' 'bc57a667-372a-587e-7c48-a3036c1bc7e8'\n",
      " 'c18a2b16-4f97-64ba-3561-61300db387a1' '2aa9e645-0073-5866-ae58-2943b4642d32' '7368cb71-43d8-450e-aacd-0863bdd88c02' 'd821a391-e77d-3f85-7f72-4a28e5e4cd3a' '83a1f729-06f8-a9f2-0518-d8c45f442812']\n",
      "Ground truth \t ['035fd7fd-70d7-8e73-43e3-b0fa0bdbbcc2' '058c2047-cdb9-6ce8-8130-80e53f23622f' '07229899-48c9-ad8d-1fdf-58657f69dcab' '0a7bb3d0-4762-cee3-e4f2-f7d1d153ddd2' '0c70af68-595f-3125-ed90-a74a2a8fb0f7' '0cf23d2f-bada-652a-3325-a3298a52d1c5' '0d0160ac-3076-83fc-76ca-7bf0189d86e1' '0e35135a-e5a5-ac85-1867-78d694cc3d86'\n",
      " '0f46e82c-b6a7-5819-9852-003f6861922c' '0fca832e-87ea-029e-4a39-0c1ae5a49416' '110a0bd4-5393-a07b-62d2-5cf0874f4b67' '13358d13-b940-7836-2d35-d00e8310b297' '144aac36-83d9-d66e-535b-b4eb29c8da7e' '175ce139-96a2-9ac9-27fe-d07f628034ad' '176a0208-db4d-26fc-5578-f38449c27981' '18b0f340-8147-3d7f-a8aa-22d590108a1b'\n",
      " '19fa3e36-bb68-3fbc-625d-a7d788796ac4' '1cf57545-7a91-ab69-4ecc-5ef84fadd959' '1f8d3e82-b841-5def-bc2c-e87c8ddf0dd7' '21027863-1d99-20d5-e7f9-dd3f7ad3b166' '22b52713-6ddf-36a4-55f2-68d76f7a6798' '23ee60ca-68d4-e54f-183c-369af122897b' '24e404dd-094c-e5bd-defa-e52125422443' '252c13f3-4bbe-8e1e-9569-295a5fb4c1b7'\n",
      " '2583f9b6-1b10-2c5f-2136-62badc34dcf2' '262a2330-5e47-8311-180d-920d61e42841' '26560de1-6930-ddaf-5069-f7b85acd40fb' '2aa9e645-0073-5866-ae58-2943b4642d32' '2fa8a4bb-118b-432f-3993-caa3ab7b931c' '341569f2-1f34-4884-1dd3-79137be4c77f' '354414a0-35b5-d795-975c-1e967b3b86a6' '35aa73bf-5bdc-5647-0126-d67cf3b3fa7a'\n",
      " '37ca86f3-c055-6935-e396-70403d4cab46' '3aef4a5f-9c50-80bd-b420-1b29c4e66fe9' '3c66087a-5c7d-464e-3bed-d5797a34ad71' '3ebff1ce-2822-188c-ab37-14f36f30fbd6' '3f47e775-77a5-845f-6604-7e11952d083d' '42c43f87-d748-78db-7980-2d5545ebc06e' '431741cc-839f-cbaf-9859-162abfa36ba4' '48501ab8-4a0f-8ce0-823b-d0dd26e4098b'\n",
      " '4b0262ab-61cc-fdbf-8a08-320c9052963c' '4bf26c00-3a83-a27e-c042-064ce1a02201' '4fe4aa19-11cb-fa79-5d7e-43b2014dc286' '535ee47e-9a8d-156b-439e-00ceb0bc08ec' '55d7f55c-6be0-79a5-8726-69b723afbbae' '564d584d-61a8-dd33-2be9-80608a792c7d' '56b9a02f-2373-b64c-ed63-5ac7cbf788bf' '59af5b4c-cd5c-9d48-0f2c-7c390ffec1c3'\n",
      " '61f6281f-4a09-dbd1-8d89-4e1bcfb7e50b' '65b576f8-38e9-66ff-c614-20f3c81dd0ff' '672dbdf0-ace0-4c81-6e45-25ab615fe9e9' '67e6aff7-0b58-eb98-ef49-e543a31f27d3' '687318c6-fe9c-3f7f-8983-3c61cb5b6811' '69f95579-8a42-8d93-8d78-7b641952933c' '6c42f6ee-96d8-ce7c-4594-d68850806cc3' '71039af3-3766-b597-d579-ae26cf995a9a'\n",
      " '722ebe70-ff1e-a7c4-2c16-b291abb3c996' '7368cb71-43d8-450e-aacd-0863bdd88c02' '75e0b508-e0b7-0883-885c-ef049b83defb' '77ec3c66-4342-b777-89fd-79f15a5f0c6b' '7822d234-4294-9223-e711-39feefe6e319' '791b3b3e-e556-6d31-ff6d-860f210addc0' '7bb52b6a-23af-e0ac-1eed-ac1e6e74c90f' '7c8a623d-c9cf-0844-f844-b2ed52063b87'\n",
      " '7cd6c6f1-72cf-fa6e-00c0-84b7534ed728' '83a1f729-06f8-a9f2-0518-d8c45f442812' '847cef24-2a3c-af15-2e58-ec1046dbf155' '899625f6-0b4f-3266-75af-607b0d8c04b9' '8b066c82-4140-4c42-c35a-12da4cd1ae17' '8c85bd9e-b53d-38f5-9ae5-e6bb42cf939f' '905a0cdc-495e-b58c-6f70-a7212047a1fa' '92cd1fa4-9cdb-9925-f0b2-1cf17f1e1199'\n",
      " '95d65d3c-d851-6fdf-2d80-b91ef84459cf' '95e2f184-e765-da67-9e88-2a13bb3de53b' '9699586e-38ef-27da-a531-94f5b1d6934b' '996d941d-79fa-885a-e12b-00200c7e61f0' '9ca2397d-b776-395e-b78f-28b3ae2934dd' '9cc4a537-c0a2-3d06-3780-6b327d292303' '9e360c34-66c0-5039-8b7b-2ffbf70605a1' 'a13993a7-cc44-77c6-2122-93171548125d'\n",
      " 'a2bbb21c-473f-74a9-15e2-b6661457ac0c' 'a785af89-b8c0-5e7b-acec-c4874ec5483f' 'a7a1965c-6c33-b274-3127-f1fcd3ed73b1' 'a9429b5d-fe9f-08f9-3f79-c6497a00fcfc' 'abacc294-6ad6-a62e-c2a4-ce2394b54ae5' 'adde4f5e-741e-76b2-0950-feb593c4f41b' 'ae6c3719-b4e6-e9d3-d3b0-6931138c8c5e' 'b1a2b05b-55a9-e93a-4e04-8cef85c1be09'\n",
      " 'b1a459de-6869-191f-3650-c345bd1552d4' 'b2bb892c-9e2d-0ceb-a7a6-d81c90b937cf' 'b4d1442b-926b-1002-f13d-467a27386744' 'b68ab3d0-ce37-6c24-42ab-48968dea7b26' 'b80f199f-2ead-a809-6a2d-1cf9fccd37a3' 'bae687a1-c9cc-2c4f-5cdf-f567f9c2fab1' 'bb27ae2e-1c8a-9a2b-e28f-eb8d05360bda' 'bc57a667-372a-587e-7c48-a3036c1bc7e8'\n",
      " 'bca7d3ec-938a-fdc4-aeda-51ac609cf255' 'bf0c7f5f-8b66-c0b2-633f-c78a1558f7c9' 'c0ff5120-0537-81d2-6616-d7852ac605bf' 'c56c16c4-c330-b17a-367f-7c04350a71bd' 'c971740e-f359-97ec-aed3-1773e5e6bbac' 'cc9f79ae-5c41-7f63-b237-f5d699d731ed' 'd638c399-ff9f-b83d-3fe3-e533b881aa1d' 'd71c025f-0397-1ff1-7514-63896e72e6f3'\n",
      " 'db4a90ce-843b-3ccd-5da7-ace6530d748d' 'de473d56-8c61-13cb-6684-933726b40a3d' 'de7ec2ae-527d-f061-0ca2-b66f1ad852ff' 'e125d3f5-fd3f-af8a-42f7-5cf01ae299a7' 'e24ef44c-ab6e-ab10-c974-83f93a2a7507' 'e26305d0-d508-dc76-61d6-a7372111bd37' 'e376b53f-b639-dcfb-742b-a6caf8494c42' 'e48a123e-8c69-23b8-6f0c-f3f8ef71aa73'\n",
      " 'e51ca99b-7f28-af3c-2fde-51e70deb5ab7' 'e6206502-d6fd-20fe-8af8-0f59f05228d3' 'e7515a92-d708-9705-8da7-34dbc4ef990d' 'ebc20e8a-25f5-5895-0190-deb630e647ca' 'ecec2054-01c7-ed9d-94c6-603a3836f2f2' 'ee21808a-5a63-c64d-d8f9-744d685abb8c' 'efbedb66-d0e1-a75c-61ed-e37b664b3031' 'f1490c88-b73e-4566-e948-d2255d0770a3'\n",
      " 'f18cf0dd-b72e-9706-51ef-1abfb25847a2' 'f8433e57-cab6-66d3-4b86-87aa9b50fc38' 'f93c2b81-f661-2750-dc0e-449950c6eba2' 'fb00d262-d8b7-13dc-0cf7-2cd99c42b5a8' 'fd39063f-b353-c2af-99a7-0c87114472c9']\n",
      "\n",
      " Accuracy:  0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/amee/Documents/code/master-thesis/EagleID/notebooks/../utils/triplet_loss_utils.py:80: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  results = pd.DataFrame(results).T.fillna(method=\"ffill\").T\n"
     ]
    }
   ],
   "source": [
    "from utils.triplet_loss_utils import KnnClassifier\n",
    "from wildlife_tools.features import DeepFeatures\n",
    "\n",
    "query_loader, gallery_loader = data.val_dataloader()\n",
    "gallery_dataset = gallery_loader.dataset\n",
    "query_dataset = query_loader.dataset\n",
    "\n",
    "backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n",
    "extractor = DeepFeatures(backbone)\n",
    "query, database = extractor(query_dataset), extractor(gallery_dataset)\n",
    "\n",
    "print(f'Query features shape: {query.shape}, Database features shape: {database.shape}')\n",
    "# Cosine similarity between deep features\n",
    "similarity_function = CosineSimilarity()\n",
    "similarity = similarity_function(query, database)['cosine']\n",
    "print(\"Similarity matrix: \\n\", similarity.shape)\n",
    "\n",
    "### Debug: Check indices and sizes\n",
    "print(f\"Database labels map size: {len(gallery_dataset.labels_map)}\")\n",
    "print(f\"Sample indices from similarity: {np.argmax(similarity, axis=1)[:10]}\")\n",
    "\n",
    "# Nearest neigbour classifier using the similarity\n",
    "classifier = KnnClassifier(k=1, database_labels=gallery_dataset.labels)\n",
    "print(f\"size of gallery dataset labels: {len(gallery_dataset.labels)}\")\n",
    "preds = classifier(similarity)\n",
    "preds = gallery_dataset.labels_map[preds]\n",
    "print(\"Prediction \\t\", preds)\n",
    "print(\"Ground truth \\t\", query_dataset.labels_string)\n",
    "\n",
    "acc = sum(preds == query_dataset.labels_string) / len(query_dataset.labels_string)\n",
    "print('\\n Accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[{'val/mAP1': 0.0, 'val/mAP5': 0.004666666500270367},\n",
    " {'val/mAP1': 0.0, 'val/mAP5': 0.004666666500270367}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = yaml.safe_load(args.config)\n",
    "with open(config_file_path, 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "data = ArtportalenDataModule(data_dir=config['dataset'], preprocess_lvl=config['preprocess_lvl'], batch_size=config['batch_size'], size=config['img_size'], mean=config['transforms']['mean'], std=config['transforms']['std'])\n",
    "data.prepare_testing_data(config['dataset'])\n",
    "dataloader = data.test_dataloader()\n",
    "\n",
    "model = SimpleModel(config=config, pretrained=False, num_classes=data.num_classes)\n",
    "if args.gpu:\n",
    "    checkpoint = torch.load(config['checkpoint'])\n",
    "else:\n",
    "    checkpoint = torch.load(config['checkpoint'], map_location=torch.device('cpu'), weights_only=True)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.to(torch.device('cpu'))\n",
    "\n",
    "\n",
    "trainer = Trainer(accelerator=\"cpu\")\n",
    "# trainer.fit(model, data)\n",
    "trainer.test(model, dataloaders=dataloader, ckpt_path=config['checkpoint'])\n",
    "trainer.validate(model, dataloaders=data.val_dataloader())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
