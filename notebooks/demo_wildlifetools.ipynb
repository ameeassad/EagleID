{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import chain\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# import timm\n",
    "# import pandas as pd\n",
    "import torchvision.transforms as T\n",
    "# from torch.optim import SGD\n",
    "\n",
    "# from wildlife_tools.data import WildlifeDataset, SplitMetadata\n",
    "# from wildlife_tools.train import ArcFaceLoss\n",
    "# from wildlife_tools.train import BasicTrainer\n",
    "\n",
    "\n",
    "# import timm\n",
    "# import numpy as np\n",
    "# from wildlife_datasets.datasets import WhaleSharkID\n",
    "from wildlife_tools.data import WildlifeDataset\n",
    "# import torchvision.transforms as T\n",
    "# from wildlife_datasets import datasets, splits\n",
    "# from wildlife_tools.features import DeepFeatures\n",
    "# from wildlife_tools.similarity import CosineSimilarity\n",
    "from wildlife_tools.inference import KnnClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# import timm\n",
    "import itertools\n",
    "from torch.optim import SGD\n",
    "# from wildlife_tools.train import ArcFaceLoss, BasicTrainer , TripletLoss\n",
    "from utils.trainer_pl import basic_trainer_pl\n",
    "from models.template_model import TemplateModel\n",
    "from utils.triplet_loss_utils import TripletLoss_wildlife\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import argparse\n",
    "import shutil\n",
    "import os\n",
    "import yaml\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_lightning import Trainer\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import wandb\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from wildlife_datasets import analysis, datasets, loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/amee/miniconda3/envs/pytorch_env/lib/python3.12/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name        | Type                 | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model       | SwinTransformer      | 27.5 M | train\n",
      "1 | loss_module | TripletLoss_wildlife | 0      | train\n",
      "-------------------------------------------------------------\n",
      "27.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.5 M    Total params\n",
      "110.077   Total estimated model params size (MB)\n",
      "/Users/amee/miniconda3/envs/pytorch_env/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "/Users/amee/miniconda3/envs/pytorch_env/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (43) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=0` reached.\n"
     ]
    }
   ],
   "source": [
    "root = '/Users/amee/Documents/code/master-thesis/datasets/ATRW/'\n",
    "\n",
    "# Load dataset metadata\n",
    "metadata = datasets.ATRW(root)\n",
    "transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
    "dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "# Download MegaDescriptor-T backbone from HuggingFace Hub\n",
    "backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n",
    "\n",
    "# Arcface loss - needs backbone output size and number of classes.\n",
    "objective = TripletLoss_wildlife()\n",
    "\n",
    "# Optimize parameters in backbone and in objective using single optimizer.\n",
    "params = itertools.chain(backbone.parameters(), objective.parameters())\n",
    "optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n",
    "\n",
    "def print_epoch_loss(trainer, epoch_data):\n",
    "    # This function will print the average loss at the end of each epoch\n",
    "    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n",
    "\n",
    "\n",
    "trainer = basic_trainer_pl(\n",
    "    dataset=dataset,\n",
    "    model=backbone,\n",
    "    objective=objective,\n",
    "    optimizer=optimizer,\n",
    "    epochs=0,\n",
    "    device='cpu',\n",
    "    epoch_callback=print_epoch_loss\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:13<00:00, 13.98s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 42/42 [05:46<00:00,  8.26s/it]\n"
     ]
    }
   ],
   "source": [
    "from wildlife_tools.features import DeepFeatures\n",
    "\n",
    "\n",
    "dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n",
    "dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)\n",
    "\n",
    "# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n",
    "extractor_P = DeepFeatures(backbone , device = 'cpu')\n",
    "\n",
    "query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cosine': array([[ 0.02134773, -0.10590538,  0.02437633, ...,  0.03205836,\n",
      "         0.04964858,  0.0191994 ],\n",
      "       [ 0.08248976, -0.02898927, -0.04742883, ..., -0.00551586,\n",
      "        -0.07753257,  0.0472618 ],\n",
      "       [-0.07289029, -0.02074489,  0.14086668, ...,  0.05982637,\n",
      "        -0.03907153, -0.08189144],\n",
      "       ...,\n",
      "       [ 0.03932605,  0.10952195,  0.02927184, ...,  0.15346071,\n",
      "         0.08338239,  0.10213841],\n",
      "       [ 0.00754523,  0.03353639,  0.08925048, ...,  0.00589957,\n",
      "         0.05694124,  0.06781136],\n",
      "       [ 0.11477285,  0.0210886 ,  0.03463744, ...,  0.01759844,\n",
      "        -0.01871965,  0.02832869]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "from utils.triplet_loss_utils import KnnClassifier\n",
    "from wildlife_tools.similarity import CosineSimilarity\n",
    "\n",
    "similarity_function = CosineSimilarity()\n",
    "similarity_P = similarity_function(query_P, database_P)\n",
    "print(similarity_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "5315\n",
      "Predictions for 100 test Images:-\n",
      " ['16' '34' '177' '18' '112' '18' '19' '144' '177' '153' '205' '147' '208'\n",
      " '166' '27' '243' '109' '163' '169' '178' '270' '96' '12' '234' '250'\n",
      " '211' '156' '190' '4' '193' '240' '156' '57' '10' '112' '169' '103' '97'\n",
      " '261' '120' '169' '246' '172' '70' '0' '199' '267' '112' '156' '118' '15'\n",
      " '121' '162' '159' '159' '90' '141' '213' '48' '246' '166' '162' '252'\n",
      " '93' '73' '84' '18' '39' '244' '192' '268' '100' '166' '255' '244' '249'\n",
      " '31' '177' '48' '136' '162' '64' '220' '141' '207' '7' '162' '162' '84'\n",
      " '196' '21' '157' '147' '162' '177' '123' '156' '88' '238' '157']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amee/Documents/code/master-thesis/EagleID/notebooks/../utils/triplet_loss_utils.py:138: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  results = pd.DataFrame(results).T.fillna(method=\"ffill\").T\n"
     ]
    }
   ],
   "source": [
    "classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n",
    "predictions_P = classifier_P(similarity_P['cosine'])\n",
    "print(\"Predictions for 100 test Images:-\\n\",predictions_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on ATRW data: 95.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n",
    "print(\"Accuracy on ATRW data: {:.2f}%\".format(accuracy_P * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9883333333333333\n",
      "Recall: 0.95\n",
      "F1 Score: 0.9493333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n",
    "recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n",
    "f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n",
    "print(\"Precision:\", precision_P)\n",
    "print(\"Recall:\", recall_P)\n",
    "print(\"F1 Score:\", f1_P)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
