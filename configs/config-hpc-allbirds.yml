# BEFORE RUN, CHECK: cache_path, splitter, precompute, arcface_loss.activate, epochs

project_name: eagleID-dataset-scope
notes: allbirds
use_wandb: True
wandb_entity: winniethepooh

wildlife_name: raptors, BirdIndividualID
animal_cat: bird, bird
outdir: results
dataset: /proj/nobackup/aiforeagles/
cache_path: /proj/nobackup/aiforeagles/EagleID/dataset/dataframe/cache_allbirds_split.csv # _split.csv if using metadata_split
splitter: metadata_split # custom_closed / metadata_split 
split_ratio: 0.8

only_cache: True # if only_cache, will not run mmpose inference / segmentation and will only use cache results
precompute: True # PREREQUISITE: Either delete files first or use metadata_split!

model_architecture: ResNetPlusModel # ResNetPlusModel, TripletModel, FusionModel, EfficientNet, TransformerModel
backbone_name: resnet50 # efficientnet_b0, resnet50, swin_tiny_patch4_window7_224
checkpoint: 
preprocess_lvl: 2 # 0: no preprocess, 1: bbox, 2: mask, 3: skeleton, 4: components, 5: heatmaps

batch_size: 64 # 32, 64, 128 (Larger can provide more diverse triplets in each batch -> impprove quality of mined triplets)
img_size: 224 # 224, 256, 384, 512
epochs: 20
save_interval: 10
n_gpu: 1
num_workers: 8
gpu_ids: 
seed: 7 #42, 1234, 7, 2025

use_gradcam: True
val_viz: True

transforms:
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]
classic_transform: False

embedding_size: 512 # choose between: 128, 256, 512, 768, 2048

triplet_loss:
  margin: 0.5 # choose between: 0.2, 0,5, 0.7 (higher encourages harder triplet mining and stronger gradients)
  mining_type: semihard # choose between: semihard, hard (more challenging triplets)
  distance_matrix: cosine
arcface_loss:
  activate: True # Set to False if using triplet loss
  margin: 0.5 # 1,2:[0.5,30], 3:[0.3,50], 4,5:[0.4,50] ; multispecies:[0.3,50]
  scale: 30
  theta_zero: 0.785 # ArcFace Sub-Center Dynamic Loss
  n_classes:
early_stopping:
  enabled: False
  monitor: val/mAP
  min_delta: 0.0005
  patience: 50
  verbose: true
  mode: min

solver:
  OPT: adam  # adam, sgd
  WEIGHT_DECAY: 0.0001 # 0.001, 0.0001 (small dataset)
  MOMENTUM: 0.9  # only when OPT is sgd
  BASE_LR: 0.001 # if SGD use 0.01 / if Adam use 0.001
  LR_SCHEDULER: cosine_annealing  # step, multistep, reduce_on_plateau, cosine_annealing
  LR_DECAY_RATE: 0.1
  LR_STEP_SIZE: 10  # only when LR_SCHEDULER is step
  LR_STEP_MILESTONES: [20, 30]  # only when LR_SCHEDULER is multistep
  
  use_swa: False #Stochastic Weight Averaging (SWA) to help improve generalization
  swa_lr: 0.035
  swa_start: 24
  lr_max: 0.0001
  lr_start: 3e-06
  lr_ramp_ep: 0

re_ranking: False
