project_name: local-eagleID-whaleshark
notes: 
use_wandb: False
wandb_entity: winniethepooh

wildlife_name: whaleshark
animal_cat: fish
outdir: ./results
dataset: /Users/amee/Documents/code/master-thesis/datasets/EDA-whaleshark/
cache_path: ./dataset/dataframe/cache_whaleshark.csv
splitter: closed
split_ratio: 0.8

only_cache: True # if only_cache, will not run mmpose inference / segmentation and will only use cache results

model_architecture: EfficientNet # TripletModel, FusionModel, EfficientNet, TransformerModel
backbone_name: efficientnet_b0 # swin_tiny_patch4_window7_224, efficientnet_b0, resnet18
checkpoint: 
preprocess_lvl: 3 # 0: no preprocess, 1: bbox, 2: mask, 3: skeleton, 4: components, 5: heatmaps

batch_size: 32 # 32, 64, 128 (Larger can provide more diverse triplets in each batch -> impprove quality of mined triplets)
img_size: 224 # 224, 256, 384, 512
epochs: 2
save_interval: 10
n_gpu: 0
num_workers: 2
gpu_ids: 
seed: 42

use_gradcam: True

transforms:
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]

embedding_size: 128 # choose between: 128, 256, 512, 768, 2048

triplet_loss:
  margin: 0.5 # choose between: 0.2, 0,5, 0.7 (higher encourages harder triplet mining and stronger gradients)
  mining_type: semihard # choose between: semihard, hard (more challenging triplets)
  distance_matrix: euclidean
arcface_loss:
  margin: 0.328
  scale: 49.326
  theta_zero: 0.785 # ArcFace Sub-Center Dynamic Loss
  n_classes:

early_stopping:
  enabled: False
  monitor: val/loss
  min_delta: 0.0005
  patience: 50
  verbose: true
  mode: min

solver:
  OPT: adam  # adam, SGD
  WEIGHT_DECAY: 0.0001 # 0.001, 0.0001 (small dataset)
  MOMENTUM: 0.9  # only when OPT is sgd
  BASE_LR: 0.0001
  LR_SCHEDULER: step  # step, multistep, reduce_on_plateau
  LR_DECAY_RATE: 0.1
  LR_STEP_SIZE: 10  # only when LR_SCHEDULER is step
  LR_STEP_MILESTONES: [20, 30]  # only when LR_SCHEDULER is multistep
  
  use_swa: False #Stochastic Weight Averaging (SWA) to help improve generalization
  swa_lr: 0.035
  swa_start: 24
  lr_max: 0.0005
  lr_start: 1.71e-06
  lr_ramp_ep: 0
  lr_decay: 0.8

re_ranking: False