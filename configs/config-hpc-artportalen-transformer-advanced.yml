# Configuration for Artportalen dataset with Transformer model and advanced augmentations
# This config uses advanced augmentations (MixUp, CutMix, Random Erasing, AutoAugment) for transformer models

# Project settings
project_name: eagle-ageclassifier
notes: Transformer-based age classification with 5 classes and advanced augmentations
use_wandb: True
wandb_entity: winniethepooh
outdir: results

# Model configuration
model_architecture: TransformerCategory
backbone_name: 'vit_base_patch16_224'  # Vision Transformer base model
num_classes: 5
checkpoint: 

# Dataset configuration
wildlife_name: artportalen
animal_cat: bird
dataset: '/proj/nobackup/aiforeagles/artportalen/artportalen_goeag/'
cache_path: '/proj/nobackup/aiforeagles/EagleID/dataset/dataframe/cache_artportalen.csv'
cache_dir: '/proj/nobackup/aiforeagles/EagleID/dataset/artportalen_cache'
preprocess_lvl: 2  # Masked images
img_size: 224  # Standard size for ViT models

# Training configuration
epochs: 100
batch_size: 32
save_interval: 10
n_gpu: 1
num_workers: 8
gpu_ids: 
seed: 42

# Data augmentation (advanced for transformer models)
transforms:
  mean: [0.485, 0.456, 0.406]  # ImageNet normalization
  std: [0.229, 0.224, 0.225]
  use_advanced_aug: true  # Enable advanced augmentations

# Optimized classifier hyperparameters for 5 classes
classifier:
  dropout_rate1: 0.1  # Reduced dropout for simpler head
  hidden_dim: 512     # Hidden dimension
  label_smoothing: 0.1 # Label smoothing for better generalization

# Solver configuration (required by train.py)
solver:
  OPT: adamw  # adamw for transformers
  WEIGHT_DECAY: 0.01 # Higher weight decay for transformers
  MOMENTUM: 0.9  # only when OPT is sgd
  BASE_LR: 0.0001 # Lower learning rate for transformers
  LR_SCHEDULER: cosine_annealing  # step, multistep, reduce_on_plateau, cosine_annealing
  LR_DECAY_RATE: 0.1
  LR_STEP_SIZE: 10  # only when LR_SCHEDULER is step
  LR_STEP_MILESTONES: [30, 50, 70]  # only when LR_SCHEDULER is multistep
  
  use_swa: False #Stochastic Weight Averaging (SWA) to help improve generalization
  swa_lr: 0.035
  swa_start: 24
  lr_max: 0.0001
  lr_start: 1e-06
  lr_ramp_ep: 0

# Early stopping
early_stopping:
  enabled: True
  monitor: val/acc
  min_delta: 0.01
  patience: 15
  verbose: true
  mode: max

# Other settings
use_gradcam: False
val_viz: False
only_cache: True
re_ranking: False

# Wandb logging
wandb:
  project: 'eagle-id'
  name: 'artportalen-transformer-advanced'
  tags: ['artportalen', 'transformer', 'advanced-aug', 'age-classification'] 