# Configuration for Artportalen dataset with Optimized Transformer model
# This config addresses common transformer training issues on small datasets

# Project settings
project_name: eagle-ageclassifier
notes: Optimized Transformer-based age classification with improved training strategies
use_wandb: True
wandb_entity: winniethepooh
outdir: results

# Model configuration
model_architecture: TransformerCategory
backbone_name: 'vit_base_patch16_224'  # Vision Transformer base model
num_classes: 5
checkpoint: 

# Dataset configuration
wildlife_name: artportalen
animal_cat: bird
dataset: '/proj/nobackup/aiforeagles/artportalen/artportalen_goeag/'
cache_path: '/proj/nobackup/aiforeagles/EagleID/dataset/dataframe/cache_artportalen.csv'
cache_dir: '/proj/nobackup/aiforeagles/EagleID/dataset/artportalen_cache'
preprocess_lvl: 2  # Masked images
img_size: 224  # Standard size for ViT models

# Training configuration
epochs: 100
batch_size: 32
save_interval: 10
n_gpu: 1
num_workers: 8
gpu_ids: 
seed: 42

# Data augmentation (controlled for transformer models)
transforms:
  mean: [0.485, 0.456, 0.406]  # ImageNet normalization
  std: [0.229, 0.224, 0.225]
  use_advanced_aug: false  # Disable advanced augmentations for first 10 epochs

# Enhanced classifier hyperparameters for better regularization
classifier:
  dropout_rate1: 0.3  # Increased dropout for better regularization
  dropout_rate2: 0.0  # Additional dropout layer
  hidden_dim: 512     # Hidden dimension
  label_smoothing: 0.0  # Remove label smoothing for small class count (5 classes)

# Solver configuration optimized for transformers
solver:
  OPT: adamw  # adamw for transformers
  WEIGHT_DECAY: 0.05  # Higher weight decay for better regularization
  MOMENTUM: 0.9  # only when OPT is sgd
  BASE_LR: 0.0003  # Lower learning rate for transformers
  LR_SCHEDULER: cosine_with_warmup  # New scheduler with warmup
  LR_DECAY_RATE: 0.1
  LR_STEP_SIZE: 10  # only when LR_SCHEDULER is step
  LR_STEP_MILESTONES: [30, 50, 70]  # only when LR_SCHEDULER is multistep
  
  # New transformer-specific parameters
  WARMUP_EPOCHS: 5  # Warmup period before aggressive augmentation
  WARMUP_FREEZE: 2  # Freeze backbone for first 5 epochs
  MIN_LR: 1e-6  # Minimum learning rate
  LAYER_DECAY: 0.65  # Layer-wise learning rate decay factor
  HEAD_LR_MULTIPLIER: 10  # Head learning rate multiplier (5x higher than backbone)
  
  use_swa: False #Stochastic Weight Averaging (SWA) to help improve generalization
  swa_lr: 0.035
  swa_start: 24
  lr_max: 0.0001
  lr_start: 1e-06
  lr_ramp_ep: 0

# Early stopping with better monitoring
early_stopping:
  enabled: True
  monitor: val/loss 
  min_delta: 0.01
  patience: 15
  verbose: true
  mode: max

# Regularization configuration
regularization:
  # Backbone regularization
  drop_rate: 0.1  # Dropout in backbone
  drop_path_rate: 0.1  # Stochastic depth
  
  # Gradient clipping
  gradient_clip_val: 1.0

# Other settings
use_gradcam: False
val_viz: False
only_cache: True
re_ranking: False

# Wandb logging
wandb:
  project: 'eagle-id'
  name: 'artportalen-transformer-optimized'
  tags: ['artportalen', 'transformer', 'optimized', 'age-classification', 'llrd', 'warmup', 'freeze']

# Training Strategy Notes:
# 1. Lower learning rate (1e-4) with warmup for stable training
# 2. Higher weight decay (0.05) for better regularization
# 3. Layer-wise learning rate decay (0.65) for backbone vs classifier
# 4. Removed label smoothing (0.0) for small class count
# 5. Aggressive augmentation only after warmup period (controlled in model)
# 6. Enhanced classifier with additional layers and normalization
# 7. Stochastic depth and dropout in backbone for regularization
# 8. Monitor F1 score instead of accuracy for better model selection
# 9. Freeze backbone for first 5 epochs to let classifier learn first
# 10. Head learning rate 5x higher than backbone for better convergence 