# BEFORE RUN, CHECK: cache_path, splitter, precompute, arcface_loss.activate, epochs

project_name: eagleID-preprocess-lvls
notes: 
use_wandb: True
wandb_entity: winniethepooh

wildlife_name: raptors
animal_cat: bird
outdir: results
dataset: /proj/nobackup/aiforeagles/raptor_individuals_cropped/
cache_path: /proj/nobackup/aiforeagles/EagleID/dataset/dataframe/cache_raptors_split.csv # _split.csv if using metadata_split
splitter: metadata_split # custom_closed / metadata_split 
split_ratio: 0.8

only_cache: True # if only_cache, will not run mmpose inference / segmentation and will only use cache results
precompute: True

model_architecture: ResNetPlusModel # ResNetPlusModel, TripletModel, FusionModel, EfficientNet, TransformerModel
backbone_name: resnet50 # efficientnet_b0, resnet50, swin_tiny_patch4_window7_224
checkpoint: 
preprocess_lvl: 2 # 0: no preprocess, 1: bbox, 2: mask, 3: skeleton, 4: components, 5: heatmaps

batch_size: 64 # 32, 64, 128 (Larger can provide more diverse triplets in each batch -> impprove quality of mined triplets)
img_size: 224 # 224, 256, 384, 512
epochs: 60
save_interval: 10
n_gpu: 1
num_workers: 8
gpu_ids: 
seed: 42

use_gradcam: True
val_viz: False

transforms:
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]
classic_transform: False

embedding_size: 512 # choose between: 128, 256, 512, 768, 2048

triplet_loss:
  margin: 0.5 # choose between: 0.2, 0,5, 0.7 (higher encourages harder triplet mining and stronger gradients)
  mining_type: semihard # choose between: semihard, hard (more challenging triplets)
  distance_matrix: cosine
arcface_loss:
  activate: True # Set to False if using triplet loss
  margin: 0.5 # 1,2:[0.5,50], 3:[0.3,30], 4:[0.5,64], 5:[0.3,64]
  scale: 50 
  theta_zero: 0.785 # ArcFace Sub-Center Dynamic Loss
  n_classes:

early_stopping:
  enabled: False
  monitor: val/mAP
  min_delta: 0.01
  patience: 20
  verbose: true
  mode: max

solver:
  OPT: adam  # adam, sgd
  WEIGHT_DECAY: 0.0001 # 0.001, 0.0001 (small dataset)
  MOMENTUM: 0.9  # only when OPT is sgd
  BASE_LR: 0.001 # if SGD use 0.01 / if Adam use 0.001
  LR_SCHEDULER: multistep  # step, multistep, reduce_on_plateau, cosine_annealing
  LR_DECAY_RATE: 0.9
  LR_STEP_SIZE: 10  # only when LR_SCHEDULER is step
  LR_STEP_MILESTONES: [5, 10, 15, 18, 19, 20, 21, 22, 23, 24, 25]  # only when LR_SCHEDULER is multistep
  
  use_swa: False #Stochastic Weight Averaging (SWA) to help improve generalization
  swa_lr: 0.035
  swa_start: 24
  lr_max: 0.001
  lr_start: 3e-06
  lr_ramp_ep: 8

re_ranking: True
